{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:11.480347Z",
     "iopub.status.busy": "2025-02-28T22:30:11.479965Z",
     "iopub.status.idle": "2025-02-28T22:30:14.539234Z",
     "shell.execute_reply": "2025-02-28T22:30:14.538316Z",
     "shell.execute_reply.started": "2025-02-28T22:30:11.480315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tsv_file = \"spoc-train-train.tsv\"\n",
    "csv_file = \"Train_1.csv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "df.to_csv(csv_file, index=False)\n",
    "tsv_file = \"spoc-testp.tsv\"\n",
    "csv_file = \"Train_2.csv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "df.to_csv(csv_file, index=False)\n",
    "tsv_file = \"spoc-testw.tsv\"\n",
    "csv_file = \"Train_3.csv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "df.to_csv(csv_file, index=False)\n",
    "tsv_file = \"spoc-train-eval.tsv\"\n",
    "csv_file = \"Train_4.csv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "df.to_csv(csv_file, index=False)\n",
    "tsv_file = \"spoc-train-test.tsv\"\n",
    "csv_file = \"Train_5.csv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "df.to_csv(csv_file, index=False)\n",
    "tsv_file = \"spoc-train.tsv\"\n",
    "csv_file = \"Train_6.csv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:14.540752Z",
     "iopub.status.busy": "2025-02-28T22:30:14.540443Z",
     "iopub.status.idle": "2025-02-28T22:30:16.726772Z",
     "shell.execute_reply": "2025-02-28T22:30:16.725816Z",
     "shell.execute_reply.started": "2025-02-28T22:30:14.540725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully merged and saved to merged_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_1 = 'Train_1.csv' \n",
    "train_2 = 'Train_2.csv'\n",
    "train_3 = 'Train_3.csv'\n",
    "train_4 = 'Train_4.csv'\n",
    "train_5 = 'Train_5.csv'\n",
    "train_6 = 'Train_6.csv'\n",
    "train_data_1 = pd.read_csv(train_1)\n",
    "train_data_2 = pd.read_csv(train_2)\n",
    "train_data_3 = pd.read_csv(train_3)\n",
    "train_data_4 = pd.read_csv(train_4)\n",
    "train_data_5 = pd.read_csv(train_5)\n",
    "train_data_6 = pd.read_csv(train_6)\n",
    "merged_data = pd.concat([train_data_1, train_data_2, train_data_3,train_data_4,train_data_5,train_data_6], ignore_index=True)\n",
    "merged_file = 'merged_data.csv'\n",
    "merged_data.to_csv(merged_file, index=False)\n",
    "print(f\"Data has been successfully merged and saved to {merged_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:16.728599Z",
     "iopub.status.busy": "2025-02-28T22:30:16.728276Z",
     "iopub.status.idle": "2025-02-28T22:30:17.534328Z",
     "shell.execute_reply": "2025-02-28T22:30:17.533383Z",
     "shell.execute_reply.started": "2025-02-28T22:30:16.728567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:17.535761Z",
     "iopub.status.busy": "2025-02-28T22:30:17.535527Z",
     "iopub.status.idle": "2025-02-28T22:30:17.660754Z",
     "shell.execute_reply": "2025-02-28T22:30:17.660098Z",
     "shell.execute_reply.started": "2025-02-28T22:30:17.535743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['workerid'])\n",
    "df = df.drop(columns=['probid'])\n",
    "df = df.drop(columns=['line'])\n",
    "df = df.drop(columns=['indent'])\n",
    "df = df.drop(columns=['subid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:17.661854Z",
     "iopub.status.busy": "2025-02-28T22:30:17.661563Z",
     "iopub.status.idle": "2025-02-28T22:30:20.679422Z",
     "shell.execute_reply": "2025-02-28T22:30:20.678558Z",
     "shell.execute_reply.started": "2025-02-28T22:30:17.661825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def tokenize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = text.replace(',', ' ')\n",
    "    return text.split()\n",
    "def tokenize_code(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = text.replace(',', ' ')\n",
    "    return text.split()\n",
    "\n",
    "df['tokens'] = df['text'].apply(tokenize_text)\n",
    "df['code_token'] = df['code'].apply(tokenize_code)\n",
    "df = df.drop(columns=['text'])\n",
    "df = df.drop(columns=['code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:20.681329Z",
     "iopub.status.busy": "2025-02-28T22:30:20.681031Z",
     "iopub.status.idle": "2025-02-28T22:30:22.687006Z",
     "shell.execute_reply": "2025-02-28T22:30:22.686102Z",
     "shell.execute_reply.started": "2025-02-28T22:30:20.681301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"tokenized_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Necessary Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:22.688567Z",
     "iopub.status.busy": "2025-02-28T22:30:22.688245Z",
     "iopub.status.idle": "2025-02-28T22:30:28.234620Z",
     "shell.execute_reply": "2025-02-28T22:30:28.233947Z",
     "shell.execute_reply.started": "2025-02-28T22:30:22.688525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_start_end_tokens(tokens):\n",
    "    return ['<start>'] + tokens + ['<end>']\n",
    "df['code_token'] = df['code_token'].apply(add_start_end_tokens)\n",
    "max_length = max(df['tokens'].apply(len).max(), df['code_token'].apply(len).max())\n",
    "def pad_tokens(tokens, max_length):\n",
    "    return tokens + ['<pad>'] * (max_length - len(tokens))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: pad_tokens(x, max_length))\n",
    "df['code_token'] = df['code_token'].apply(lambda x: pad_tokens(x, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:28.235488Z",
     "iopub.status.busy": "2025-02-28T22:30:28.235295Z",
     "iopub.status.idle": "2025-02-28T22:30:46.983225Z",
     "shell.execute_reply": "2025-02-28T22:30:46.982194Z",
     "shell.execute_reply.started": "2025-02-28T22:30:28.235471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"tokenized_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocablury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:30:46.984399Z",
     "iopub.status.busy": "2025-02-28T22:30:46.984120Z",
     "iopub.status.idle": "2025-02-28T22:32:46.970196Z",
     "shell.execute_reply": "2025-02-28T22:32:46.969279Z",
     "shell.execute_reply.started": "2025-02-28T22:30:46.984370Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping malformed entry: tokens\n",
      "Skipping malformed entry: code_token\n",
      "Vocabulary saved to vocabulary.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "data_path = \"tokenized_dataset.csv\"\n",
    "df = pd.read_csv(data_path, header=None, names=[\"tokens\", \"code_token\"])\n",
    "vocab = set()\n",
    "for col in [\"tokens\", \"code_token\"]:\n",
    "    for text in df[col]:\n",
    "        try:\n",
    "            words = ast.literal_eval(text)  # Safely convert string to list\n",
    "            vocab.update(words)\n",
    "        except (SyntaxError, ValueError):\n",
    "            print(f\"Skipping malformed entry: {text}\")\n",
    "special_tokens = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<UNK>\": 3}\n",
    "vocab.discard(\"<pad>\")\n",
    "vocab.discard(\"<start>\")\n",
    "vocab.discard(\"<end>\")\n",
    "vocab.discard(\"<UNK>\")\n",
    "word_to_index = {word: idx for idx, word in enumerate(sorted(vocab), start=4)}\n",
    "word_to_index = {**special_tokens, **word_to_index}\n",
    "json_path = \"vocabulary.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_to_index, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Vocabulary saved to {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:32:46.972564Z",
     "iopub.status.busy": "2025-02-28T22:32:46.972347Z",
     "iopub.status.idle": "2025-02-28T22:32:47.007980Z",
     "shell.execute_reply": "2025-02-28T22:32:47.007100Z",
     "shell.execute_reply.started": "2025-02-28T22:32:46.972544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 'create': 29038\n",
      "Decoded words: <UNK> ! !!((N !!(K\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('vocabulary.json', 'r', encoding='utf-8') as f:\n",
    "    vocab_dict = json.load(f)\n",
    "word = \"create\"\n",
    "word_index = vocab_dict.get(word, None)\n",
    "print(f\"Index of '{word}': {word_index}\")\n",
    "reverse_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "indices = [3, 4, 5, 6]  # Example word indices\n",
    "decoded_words = [reverse_vocab_dict.get(idx, \"<UNK>\") for idx in indices]\n",
    "print(f\"Decoded words: {' '.join(decoded_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:32:47.009445Z",
     "iopub.status.busy": "2025-02-28T22:32:47.009119Z",
     "iopub.status.idle": "2025-02-28T22:34:52.218481Z",
     "shell.execute_reply": "2025-02-28T22:34:52.217545Z",
     "shell.execute_reply.started": "2025-02-28T22:32:47.009413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted sequences have been saved to sequences.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "vocab_path = 'vocabulary.json'\n",
    "tokenized_data_path = 'tokenized_dataset.csv'\n",
    "output_csv_path = 'sequences.csv'\n",
    "with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "    vocabulary = json.load(f)\n",
    "def text_to_sequence(text, vocab):\n",
    "    sequence = []\n",
    "    for word in text:\n",
    "        word_cleaned = word.strip()\n",
    "        if word_cleaned in vocab:\n",
    "            sequence.append(vocab[word_cleaned])\n",
    "        else:\n",
    "            print(f\"Unmatched token: '{word_cleaned}'\")\n",
    "            sequence.append(-1)\n",
    "    return sequence\n",
    "with open(tokenized_data_path, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    output_data = []\n",
    "    for row in reader:\n",
    "        input_text = eval(row['tokens'])\n",
    "        output_text = eval(row['code_token'])\n",
    "        input_sequence = text_to_sequence(input_text, vocabulary)\n",
    "        output_sequence = text_to_sequence(output_text, vocabulary)\n",
    "        output_data.append({\n",
    "            'tokens': input_sequence,\n",
    "            'code_token': output_sequence\n",
    "        })\n",
    "with open(output_csv_path, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    fieldnames = ['tokens', 'code_token']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in output_data:\n",
    "        writer.writerow({\n",
    "            'tokens': row['tokens'],\n",
    "            'code_token': row['code_token']\n",
    "        })\n",
    "print(f\"Converted sequences have been saved to {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:34:52.219745Z",
     "iopub.status.busy": "2025-02-28T22:34:52.219425Z",
     "iopub.status.idle": "2025-02-28T22:36:45.468341Z",
     "shell.execute_reply": "2025-02-28T22:36:45.467355Z",
     "shell.execute_reply.started": "2025-02-28T22:34:52.219714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset,Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class DataLoad(Dataset):\n",
    "  def __init__(self, file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    self.inputs = [ast.literal_eval(x) for x in df['tokens']]\n",
    "    self.outputs = [ast.literal_eval(x) for x in df['code_token']]\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "  def __getitem__(self,idx):\n",
    "    input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n",
    "    output_tensor = torch.tensor(self.outputs[idx],dtype=torch.int64)\n",
    "    return input_tensor, output_tensor\n",
    "def Add_Pad(batch):\n",
    "  inputs,outputs = zip(*batch)\n",
    "  inputs = pad_sequence(inputs,batch_first=True,padding_value=0)\n",
    "  outputs = pad_sequence(outputs,batch_first=True,padding_value=0)\n",
    "  return inputs,outputs\n",
    "batch_size=32\n",
    "dataset = DataLoad('sequences.csv')\n",
    "dataloader = DataLoader(dataset,batch_size, shuffle=True,collate_fn=Add_Pad)\n",
    "data_iter = iter(dataloader)\n",
    "features, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:36:45.469565Z",
     "iopub.status.busy": "2025-02-28T22:36:45.469263Z",
     "iopub.status.idle": "2025-02-28T22:36:45.503456Z",
     "shell.execute_reply": "2025-02-28T22:36:45.502814Z",
     "shell.execute_reply.started": "2025-02-28T22:36:45.469526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocabulary.json', 'r', encoding='utf-8') as f:\n",
    "    vocab_dict = json.load(f)\n",
    "Vocab_size = len(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:36:45.504459Z",
     "iopub.status.busy": "2025-02-28T22:36:45.504239Z",
     "iopub.status.idle": "2025-02-28T22:36:45.522854Z",
     "shell.execute_reply": "2025-02-28T22:36:45.521974Z",
     "shell.execute_reply.started": "2025-02-28T22:36:45.504441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      " tensor([[52131, 45942,     0,  ...,     0,     0,     0],\n",
      "        [30196, 45942, 25334,  ...,     0,     0,     0],\n",
      "        [52131, 45942,     0,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [36884, 54530, 37953,  ...,     0,     0,     0],\n",
      "        [18997, 14193, 37805,  ...,     0,     0,     0],\n",
      "        [31005, 44428,  1956,  ...,     0,     0,     0]])\n",
      "Labels:\n",
      " tensor([[    1, 27465, 14288,  ...,     0,     0,     0],\n",
      "        [    1, 45942, 11646,  ...,     0,     0,     0],\n",
      "        [    1, 27465, 14288,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1, 36884,  9326,  ...,     0,     0,     0],\n",
      "        [    1, 37546, 18998,  ...,     0,     0,     0],\n",
      "        [    1, 28958, 14169,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Features:\\n\", features)\n",
    "print(\"Labels:\\n\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:36:45.524337Z",
     "iopub.status.busy": "2025-02-28T22:36:45.524129Z",
     "iopub.status.idle": "2025-02-28T22:36:48.018223Z",
     "shell.execute_reply": "2025-02-28T22:36:48.017502Z",
     "shell.execute_reply.started": "2025-02-28T22:36:45.524319Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=60):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, d_model=256, nhead=8, num_layers=4, dim_feedforward=512, max_len=60):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=0)\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers, \n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, output_vocab_size)\n",
    "\n",
    "    def generate_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask.to(device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = None\n",
    "        tgt_mask = self.generate_mask(tgt.size(1))\n",
    "\n",
    "        src_key_padding_mask = src == 0\n",
    "        tgt_key_padding_mask = tgt == 0\n",
    "\n",
    "        src_emb = self.positional_encoding(self.encoder_embedding(src))\n",
    "        tgt_emb = self.positional_encoding(self.decoder_embedding(tgt))\n",
    "\n",
    "        output = self.transformer(\n",
    "            src_emb.permute(1, 0, 2), \n",
    "            tgt_emb.permute(1, 0, 2),\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "        output = self.fc_out(output.permute(1, 0, 2))\n",
    "        return output\n",
    "input_vocab_size = Vocab_size  \n",
    "output_vocab_size = Vocab_size  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_vocab_size, output_vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 513398: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 513398: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "with open(\"vocabulary.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:37:18.090691Z",
     "iopub.status.busy": "2025-02-28T22:37:18.090239Z",
     "iopub.status.idle": "2025-02-28T22:37:18.138380Z",
     "shell.execute_reply": "2025-02-28T22:37:18.137605Z",
     "shell.execute_reply.started": "2025-02-28T22:37:18.090665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "with open(\"vocabulary.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "\n",
    "# Create token-to-index and index-to-token mappings\n",
    "token2idx = vocab\n",
    "idx2token = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "def tokenize(sentence, token2idx):\n",
    "    \"\"\"Tokenizes the input sentence using the vocabulary.\"\"\"\n",
    "    return [token2idx.get(token, token2idx[\"<UNK>\"]) for token in sentence.split()]\n",
    "\n",
    "def detokenize(indices, idx2token):\n",
    "    \"\"\"Converts token indices back to words.\"\"\"\n",
    "    return \" \".join([idx2token.get(idx, \"<UNK>\") for idx in indices])\n",
    "\n",
    "def predict(model, sentence, max_len=60):\n",
    "    \"\"\"Generates C++ code from pseudocode or vice versa.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input and convert to tensor\n",
    "    input_tokens = tokenize(sentence, token2idx)\n",
    "    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n",
    "\n",
    "    # Start with the <sos> token for decoding\n",
    "    output_tokens = [token2idx[\"<start>\"]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(max_len), desc=\"Decoding\", leave=True):\n",
    "            output_tensor = torch.tensor(output_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n",
    "\n",
    "            # Generate next token\n",
    "            predictions = model(input_tensor, output_tensor)  # Shape: (1, seq_len, vocab_size)\n",
    "            next_token = predictions[0, -1].argmax(dim=-1).item()\n",
    "\n",
    "            # Stop if <eos> is generated\n",
    "            if next_token == token2idx[\"<end>\"]:\n",
    "                break\n",
    "\n",
    "            # Append next token to output sequence\n",
    "            output_tokens.append(next_token)\n",
    "\n",
    "    # Convert token indices back to words\n",
    "    return detokenize(output_tokens[1:], idx2token)  # Exclude <sos> token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:37:25.275714Z",
     "iopub.status.busy": "2025-02-28T22:37:25.275425Z",
     "iopub.status.idle": "2025-02-28T23:05:02.629749Z",
     "shell.execute_reply": "2025-02-28T23:05:02.628612Z",
     "shell.execute_reply.started": "2025-02-28T22:37:25.275692Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   9%|▉         | 1854/21084 [04:23<45:27,  7.05it/s, loss=2.2110]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m         output \u001b[38;5;241m=\u001b[39m predict(model, manual_input)\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Output after Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (features, labels) \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m     25\u001b[0m     features, labels \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 27\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(features, labels[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Remove last token from labels\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:952\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m--> 952\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    954\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 10\n",
    "clip_grad = 1.0  # Gradient clipping\n",
    "batch_size = 10  # Adjust as needed\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # Set to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Wrap dataloader with tqdm for progress visualization\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for batch_idx, (features, labels) in progress_bar:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(features, labels[:, :-1])  # Remove last token from labels\n",
    "            output = output.reshape(-1, output_vocab_size)  # Reshape for loss calculation\n",
    "            labels = labels[:, 1:].reshape(-1)  # Shift labels by one position\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Update tqdm progress bar with current loss\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"\\nEpoch {epoch+1} completed in {end_time - start_time:.2f} sec, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Run prediction after each epoch\n",
    "        manual_input = \"function to add two numbers\"\n",
    "        output = predict(model, manual_input)\n",
    "        print(f\"\\nGenerated Output after Epoch {epoch+1}:\\n{output}\\n\")\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T23:05:06.576957Z",
     "iopub.status.busy": "2025-02-28T23:05:06.576616Z",
     "iopub.status.idle": "2025-02-28T23:05:07.956687Z",
     "shell.execute_reply": "2025-02-28T23:05:07.955791Z",
     "shell.execute_reply.started": "2025-02-28T23:05:06.576928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully at transformer_model.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model_save_path = \"transformer_model.pt\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "}, model_save_path)\n",
    "print(f\"✅ Model saved successfully at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model To Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T23:09:51.163400Z",
     "iopub.status.busy": "2025-02-28T23:09:51.163111Z",
     "iopub.status.idle": "2025-02-28T23:09:52.474402Z",
     "shell.execute_reply": "2025-02-28T23:09:52.473504Z",
     "shell.execute_reply.started": "2025-02-28T23:09:51.163380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_14532\\1468499665.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded from transformer_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding:   5%|▌         | 3/60 [00:00<00:00, 60.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Output:\n",
      " liter += my;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load vocabulary\n",
    "with open(\"vocabulary.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Create token-to-index and index-to-token mappings\n",
    "token2idx = vocab\n",
    "idx2token = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "# Initialize model_2 (ensure it's the same architecture as the saved model)\n",
    "model_2 = TransformerModel(input_vocab_size, output_vocab_size).to(device)  # Initialize with same parameters as trained model\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=0.001)  # Use same optimizer\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model_path, model_2, optimizer):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model_2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    model_2.eval()  # Set to evaluation mode\n",
    "    print(f\"✅ Model loaded from {model_path}\")\n",
    "\n",
    "# Load the saved model\n",
    "load_model(\"transformer_model.pt\", model_2, optimizer)\n",
    "\n",
    "# Tokenization functions\n",
    "def tokenize(sentence, token2idx):\n",
    "    \"\"\"Tokenizes the input sentence using the vocabulary.\"\"\"\n",
    "    return [token2idx.get(token, token2idx[\"<UNK>\"]) for token in sentence.split()]\n",
    "\n",
    "def detokenize(indices, idx2token):\n",
    "    \"\"\"Converts token indices back to words.\"\"\"\n",
    "    return \" \".join([idx2token.get(idx, \"<UNK>\") for idx in indices])\n",
    "\n",
    "# Prediction function\n",
    "def predict(model, sentence, max_len=60):\n",
    "    \"\"\"Generates C++ code from pseudocode or vice versa.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input and convert to tensor\n",
    "    input_tokens = tokenize(sentence, token2idx)\n",
    "    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n",
    "\n",
    "    # Start with the <sos> token for decoding\n",
    "    output_tokens = [token2idx[\"<start>\"]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(max_len), desc=\"Decoding\", leave=True):\n",
    "            output_tensor = torch.tensor(output_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n",
    "\n",
    "            # Generate next token\n",
    "            predictions = model(input_tensor, output_tensor)  # Shape: (1, seq_len, vocab_size)\n",
    "            next_token = predictions[0, -1].argmax(dim=-1).item()\n",
    "\n",
    "            # Stop if <eos> is generated\n",
    "            if next_token == token2idx[\"<end>\"]:\n",
    "                break\n",
    "\n",
    "            # Append next token to output sequence\n",
    "            output_tokens.append(next_token)\n",
    "\n",
    "    # Convert token indices back to words\n",
    "    return detokenize(output_tokens[1:], idx2token)  # Exclude <sos> token\n",
    "\n",
    "# Example input\n",
    "manual_input = 'add My'\n",
    "output = predict(model_2, manual_input)\n",
    "\n",
    "print(\"\\nGenerated Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example input\n",
    "manual_input = \"create string s\"\n",
    "output = predict(model, manual_input)\n",
    "\n",
    "print(\"\\nGenerated Output:\\n\", output)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6763385,
     "sourceId": 10884189,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
